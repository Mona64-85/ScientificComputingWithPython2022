# -*- coding: utf-8 -*-
"""07ex_visualization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l6cx6UVplRqGZZcm8WdIMjds06y2jCcj

1. Spotting correlations

Load the remote file:

https://www.dropbox.com/s/aamg1apjhclecka/regression_generated.csv
with Pandas and create scatter plots with all possible combinations of the following features:

features_1
features_2
features_3
Are these features correlated?

2\. **Color-coded scatter plot**

Produce a scatter plot from a dataset with two categories.

* Write a function that generates a 2D datasets of 2 categories. Each category should distribute as a 2D gaussian with a given mean and standard deviation. Set different values of the mean and stardand deviation between the two samples.
* Display the dataset in a scatter plot marking the two categories with different marker colors.

An example is given below:
"""

from IPython.display import Image
Image('/content/two_categories_scatter_plot.png')

import numpy as np
import matplotlib.pyplot as plt

def two_categories_data(mean1,mean2,std1,std2,size):
    x1 = np.random.normal(loc=mean1, scale=std1, size=size) #created normal random values
    y1 = np.random.normal(loc=mean1, scale=std1, size=size)
    
    
    x2 = np.random.normal(loc=mean2, scale=std2, size=size)
    y2 = np.random.normal(loc=mean1, scale=std1, size=size)
    
    
     #make it list
    return x1,y1,x2,y2




datasets_2d= two_categories_data(1,20,5,7,150)


plt.scatter(datasets_2d[0],datasets_2d[1])
plt.scatter(datasets_2d[2],datasets_2d[3])
plt.show()

N = 50
x = np.random.normal(loc=5, scale=5, size=222)
# np.arange(0,len(x_train),1)
y = np.random.normal(loc=5, scale=5, size=222)

plt.scatter(x, y)
plt.show()

"""3. Profile plot

Produce a profile plot from a scatter plot.

Download the following pickle file:
wget https://www.dropbox.com/s/3uqleyc3wyz52tr/residuals_261.pkl -P data/
Inspect the dataset, you'll find two variables (features)
Convert the content to a Pandas Dataframe
Clean the sample by selecting the entries (rows) with the absolute values of the variable "residual" smaller than 2
Plot a Seaborn jointplot of "residuals" versus "distances", and use seaborn to display a linear regression.
Comment on the correlation between these variables.

Create manually (without using seaborn) the profile histogram for the "distance" variable; choose an appropriate binning.
Obtain 3 numpy arrays:
x, the array of bin centers of the profile histogram of the "distance" variable
y, the mean values of the "residuals", estimated in slices (bins) of "distance"
err_y, the standard deviation of the of the "residuals", estimated in slices (bins) of "distance"
Plot the profile plot on top of the scatter plot
"""

import pickle
import pandas as pd
import seaborn as sns

data = pd.read_pickle("/content/residuals_261.pkl")

data_dic = data[()] #this trick makes pickle to dictionary
data_dic = pd.DataFrame(data_dic) #dic to dataframe


clean_dataframe = data_dic[(data_dic['residuals']**2)<=4] #tried to make absolute value
clean_dataframe

"""As we can see from the histograms, residuals has a very high similarity to clustered gaussian shape wherase the distances are wider because we didn't filtered them."""

sns.jointplot(data=clean_dataframe, x="residuals", y="distances", kind="reg")

# Commented out IPython magic to ensure Python compatibility.

# Comment on the correlation between these variables.

# Create manually (without using seaborn) the profile histogram for the "distance" variable; choose an appropriate binning.
# Obtain 3 numpy arrays:
# x, the array of bin centers of the profile histogram of the "distance" variable
# y, the mean values of the "residuals", estimated in slices (bins) of "distance"
# err_y, the standard deviation of the of the "residuals", estimated in slices (bins) of "distance"
# Plot the profile plot on top of the scatter plot



import matplotlib.pyplot as plt
# %matplotlib inline

# always useful
import numpy as np
nrnd = clean_dataframe['distances'] 
fig, ax = plt.subplots(figsize=(10, 6)) # create the figure and the axes
h, bins, _ = plt.hist(nrnd, bins=19, range=(0, +21), density=True, cumulative=False) # create and plot the histogram
nrnd.plot.kde(ax=ax, legend=False)

ax.set_title('Distribution')
ax.set_xlabel('x')
ax.set_ylabel('Probability')
fig.tight_layout()

# # print histogram content and bins
# print("Histogram content:", h)
# print("Bin boundaries:", bins)





"""4. Kernel Density Estimate

Produce a KDE for a given distribution (by hand, not using seaborn):

Fill a numpy array x of length N (with ) with a variable normally distributed, with a given mean and standard deviation
Fill an histogram in pyplot taking proper care of the aesthetic:
use a meaningful number of bins
set a proper y axis label
set proper value of y axis major ticks labels (e.g. you want to display only integer labels)
display the histograms as data points with errors (the error being the poisson uncertainty)
For every element of x, create a gaussian with the mean corresponding to the element value and the standard deviation as a parameter that can be tuned. The standard deviation default value should be:
 
you can use the scipy function stats.norm() for that.

In a separate plot (to be placed beside the original histogram), plot all the gaussian functions so obtained
Sum (with np.sum()) all the gaussian functions and normalize the result such that the integral matches the integral of the original histogram. For that you could use the scipy.integrate.trapz() method. Superimpose the normalized sum of all gaussians to the first histogram.
"""

means = 10
stdevs = 4
size = 100
x = np.random.normal(loc=means, scale=stdevs, size=size)
n,bins,patches = plt.hist(x=x,bins = 13)
y,binEdges = np.histogram(x,bins=13)
bincenters = 0.5*(binEdges[1:]+binEdges[:-1])
menStd     = np.sqrt(y)
width      = 0.05

# Fit a normal distribution to
# the data:
# mean and standard deviation
mu, std = norm.fit(x) 

# Plot the PDF.
xmin, xmax = plt.xlim()
lin = np.linspace(xmin, xmax, 100)
p = norm.pdf(lin, mu, std)

plt.bar(bincenters, y, width=width, color='r', yerr=menStd)

plt.grid(axis='y', alpha=0.75)


plt.text(15, 10, r'$\mu=10, b=4$')


plt.xlabel('Value')
plt.ylabel('Frequency')
plt.title('Histogram')
maxfreq = n.max()

#Kernel Density Estimate

import numpy as np
from scipy.stats import norm
import matplotlib.pyplot as plt
  
# Generate some data for this 
# demonstration.
data = x
  
# Fit a normal distribution to
# the data:
# mean and standard deviation
mu, std = norm.fit(data) 
  
# Plot the histogram.
plt.hist(data, bins=25, density=True, alpha=0.6, color='b')
  
# Plot the PDF.
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = norm.pdf(x, mu, std)
  
plt.plot(x, p, 'k', linewidth=2)
title = "Fit Values: {:.2f} and {:.2f}".format(mu, std)
plt.title(title)
  
plt.show()



